# -*- coding: utf-8 -*-
"""Accent_clean_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dIlnPrDLWy-ASvUpi5Pw480xUn3Xw08K
"""

!pip install speechbrain==0.5.15
!pip install -U yt-dlp
!pip install yt-dlp
!pip install flask pyngrok
!pip install streamlit
!pip install git+https://github.com/openai/whisper.git
!sudo apt update && sudo apt install -y ffmpeg

import librosa
import torch

from transformers import AutoModelForAudioClassification, AutoProcessor
from transformers import pipeline



import requests
from transformers import AutoFeatureExtractor

from moviepy.editor import VideoFileClip
import torchaudio
from speechbrain.pretrained.interfaces import foreign_class
import os
import soundfile as sf
import streamlit as st
import whisper
import subprocess
import uuid
from flask import Flask, request, render_template_string
from pyngrok import ngrok
from pyngrok import conf

classifier = foreign_class(source="Jzuluaga/accent-id-commonaccent_xlsr-en-english", pymodule_file="custom_interface.py", classname="CustomEncoderWav2vec2Classifier")

class Agent_classifier:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.classifier = foreign_class(
            source="Jzuluaga/accent-id-commonaccent_xlsr-en-english",
            pymodule_file="custom_interface.py",
            classname="CustomEncoderWav2vec2Classifier"
        )
        self.sampling_rate = 16000

    def predict(self, audio_path):
        return self.classifier.classify_file(audio_path)

    def predict_whole(self, audio_path):
        """Predict directly on the full audio file"""
        out_prob, score, index, text_lab = self.predict(audio_path)
        return {
          "chunk": os.path.basename(audio_path),
          "label": text_lab,         # <-- use text_lab as the readable label
          "score": score.item() if hasattr(score, 'item') else score,
          "index": index,
          "out_prob": out_prob.tolist()  # Optional: convert tensor to list if needed
          }

    def split_in_half(self, audio_path):
        """Split audio into two equal-length parts"""
        waveform, sr = librosa.load(audio_path, sr=self.sampling_rate)
        midpoint = len(waveform) // 2

        base_dir = os.path.dirname(audio_path)
        filename = os.path.splitext(os.path.basename(audio_path))[0]
        half_dir = os.path.join(base_dir, f"{filename}_halves")
        os.makedirs(half_dir, exist_ok=True)

        paths = []
        for i, (start, end) in enumerate([(0, midpoint), (midpoint, len(waveform))]):
            half = waveform[start:end]
            half_path = os.path.join(half_dir, f"{filename}_half_{i}.wav")
            sf.write(half_path, half, sr)
            paths.append(half_path)

        return paths

    def predict_halves(self, audio_path):
        """Run prediction on two halves of the audio"""
        halves = self.split_in_half(audio_path)
        results = []
        for path in halves:
            out_prob, score, index, text_lab = self.predict(path)
            results.append({
               "chunk": os.path.basename(audio_path),
              "out_prob": out_prob,
              "score": score,
              "index":index,
              "text_lab":text_lab
            })
        return results

class transcrib():
  def __init__(self):
    self.model_transcribe = whisper.load_model("base")
    self.device = "cuda" if torch.cuda.is_available() else "cpu"
  def audio_2_text(self,audio_path):
    result = self.model_transcribe.transcribe(audio_path)
    return result["text"]

url = "https://youtube.com/shorts/pv0kvjXObfg?si=brXaI54-9ksJoS55"#testing video

def download_audio_2(url, output_dir="/content/downloaded_url_videos"):
    os.makedirs(output_dir, exist_ok=True)
    uid = str(uuid.uuid4())
    output_template = os.path.join(output_dir, uid + ".%(ext)s")

    # Download audio as .wav
    command = f'yt-dlp -x --audio-format wav "{url}" -o "{output_template}"'
    subprocess.run(command, shell=True, check=True)

    # Find the actual .wav file
    files = glob.glob(os.path.join(output_dir, uid + ".*"))
    wav_files = [f for f in files if f.endswith(".wav")]

    if not wav_files:
        raise FileNotFoundError("No .wav file was downloaded.")

    return wav_files[0]  # Return the actual .wav path

def download_audio(url, output_dir="downloaded_url_videos"):
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, f"{uuid.uuid4()}.wav")
    command = f'yt-dlp -x --audio-format wav "{url}" -o "{output_path}"'
    subprocess.run(command, shell=True)
    return output_path

url_sound=download_audio(url)

agent=Agent_classifier()
results_o = agent.predict_whole(url_sound)
print(len(results_o))
print(type(results_o))
#result = agent.predict_whole(url_sound)
print(results_o)
print(results_o.values())

results_p= agent.predict_whole(url_sound)

'''
st.title("English Accent Classifier üéôÔ∏èüåç")
st.markdown("Upload a video link (e.g. YouTube, Loom, etc.) and get accent prediction.")

url = st.text_input("Enter public video URL:")

if url:
    with st.spinner("Downloading and analyzing..."):
        audio_path = download_audio(url)
        agent = Agent_classifier()
        result = agent.predict_whole(audio_path)

    st.success("Done!")
    st.markdown(f"**Accent:** `{result['label']}`")
    st.markdown(f"**Confidence Score:** `{round(result['score']*100, 2)}%`")
    st.markdown("transcribed text==>"result)

    st.markdown("**Optional Explanation:**")
    st.markdown(
        f"The model detected this as a `{result['label']}` English accent "
        f"with a confidence of `{round(result['score']*100, 2)}%`. "
        "It uses a fine-tuned wav2vec2 model trained on common English accents."
    )
    '''

pip freeze > requirements.txt

# Load a question-answering or summarization model
qa_pipeline = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
summary_pipeline = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")

from flask import session
conf.get_default().auth_token = "2ysjJgpgoAgvkzeNRUKgpH5ANGk_21zkYh6Ui4rjJDFi8aQMQ"

# Assuming your Agent_classifier and download_audio are implemented elsewhere
# from your_module import Agent_classifier, download_audio

app = Flask(__name__)

html = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>English Accent Classifier üéôÔ∏èüåç</title>
    <style>
        body {
            background: linear-gradient(to right, #e0f0ff, #f7fbff);
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
        }

        .container {
            max-width: 700px;
            margin: 60px auto;
            background-color: #ffffff;
            padding: 40px;
            border-radius: 16px;
            box-shadow: 0 4px 30px rgba(0, 123, 255, 0.15);
            text-align: center;
        }

        h2 {
            color: #0056b3;
            margin-bottom: 20px;
        }

        label {
            font-weight: bold;
            display: block;
            margin: 20px 0 5px;
            color: #003366;
            text-align: left;
        }

        input[type="text"] {
            width: 100%;
            padding: 12px;
            border: 1px solid #aacbe3;
            border-radius: 8px;
            font-size: 16px;
            background-color: #f0f8ff;
        }

        input[type="submit"] {
            margin-top: 30px;
            padding: 12px 30px;
            background-color: #007bff;
            color: white;
            font-size: 16px;
            border: none;
            border-radius: 10px;
            cursor: pointer;
            transition: background-color 0.3s ease;
        }

        input[type="submit"]:hover {
            background-color: #0056b3;
        }

        hr {
            margin: 40px 0;
            border: none;
            border-top: 1px solid #d0e4f5;
        }

        code {
            background-color: #eef6ff;
            padding: 6px 10px;
            border-radius: 6px;
            font-family: monospace;
            color: #004085;
        }

        .result-section {
            text-align: left;
            background-color: #f1f9ff;
            padding: 20px;
            border-radius: 10px;
            border: 1px solid #d6eaff;
        }

        p {
            color: #333;
        }
    </style>
</head>
<body>
    <div class="container">
        <h2> English Accent Classifier</h2>
        <p>Upload a video link (e.g. YouTube, Loom, etc.) and get accent prediction.</p>

        <form method="post">
            <label>Enter public video URL:</label>
            <input type="text" name="url" required>

            <label>Ask something about the audio:</label>
            <input type="text" name="query" placeholder="e.g. Summarize or What is being discussed?">

            <input type="submit" value="Analyze">
        </form>

        {% if result %}
            <hr>
            <div class="result-section">
                <h3>Result:</h3>
                <p><strong>Accent:</strong> {{ result['label'] }}</p>
                <p><strong>Confidence Score:</strong> {{ (result['score'] * 100) | round(2) }}%</p>
                <p><strong>Optional Explanation:</strong></p>
                <p>
                    The model detected this as a <code>{{ result['label'] }}</code> English accent
                    with a confidence of <code>{{ (result['score'] * 100) | round(2) }}%</code>.
                    It uses a fine-tuned wav2vec2 model trained on common English accents.
                </p>
            </div>
        {% endif %}

        {% if text %}
            <hr>
            <div class="result-section">
                <h3> Transcription:</h3>
                <p><code>{{ text }}</code></p>
            </div>
        {% endif %}

        {% if answer %}
            <hr>
            <div class="result-section">
                <h3> LLM Agent Response:</h3>
                <p><code>{{ answer }}</code></p>
            </div>
        {% endif %}
    </div>
</body>
</html>

"""

@app.route("/", methods=["GET", "POST"])
def classify():
    result = None
    text = None
    answer = None

    if request.method == "POST":
        url = request.form["url"]
        query = request.form.get("query", "")

        audio_path = download_audio(url)
        agent = Agent_classifier()
        transcriber = transcrib()

        result = agent.predict_whole(audio_path)
        text = transcriber.audio_2_text(audio_path)

        if query and text:
            if "summarize" in query.lower():
                answer = summary_pipeline(text[:1000])[0]["summary_text"]
            else:
                answer = qa_pipeline(question=query, context=text)["answer"]

    return render_template_string(html, result=result, text=text, answer=answer)

ngrok.kill()
public_url = ngrok.connect(5000)
print("üöÄ Open this public URL:", public_url)

app.run(port=5000)
if __name__ == "__main__":
    app.run(debug=True, port=5000)